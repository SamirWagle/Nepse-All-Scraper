# Nepsy Scraper

Scrapes NEPSE market data from ShareSansar and Merolagani for 337 priority companies.
All data is stored as CSV files in `data/` and auto-committed by GitHub Actions.

---

## ğŸ“‚ File Structure

```
scraper/
â”œâ”€â”€ run_github_actions.py   # GitHub Actions entry point (dividends, right shares, floorsheet)
â”œâ”€â”€ run_daily.py            # Local price update CLI
â””â”€â”€ core/
    â”œâ”€â”€ daily.py            # Orchestrates price scraping
    â”œâ”€â”€ daily_prices.py     # Daily price summary updater
    â”œâ”€â”€ floorsheet.py       # Daily floorsheet scraper (merolagani.com)
    â””â”€â”€ history.py          # OHLC price history scraper (incremental)
```

---

## ğŸ¤– GitHub Actions (Automated Daily)

Runs every **weekday at 6:30 PM Nepal time** via `.github/workflows/daily_scraper.yml`.

```
dividends     â†’ data/company-wise/{SYMBOL}/dividend.csv
right-shares  â†’ data/company-wise/{SYMBOL}/right-share.csv
floorsheet    â†’ data/floorsheet_YYYY-MM-DD.csv + .json
```

All output is committed and pushed back to the repo automatically.

---

## ğŸ› ï¸ Usage

### Install

```bash
pip install requests beautifulsoup4
```

### GitHub Actions script (dividends + right shares + floorsheet)

```bash
# All three
python scraper/run_github_actions.py

# Individual
python scraper/run_github_actions.py --dividends
python scraper/run_github_actions.py --right-shares
python scraper/run_github_actions.py --floorsheet

# Test floorsheet with limited pages
python scraper/run_github_actions.py --floorsheet --max-pages 3
```

### Price history (run locally)

```bash
# Incremental update â€” only fetches records newer than what's in prices.csv
python scraper/run_daily.py --incremental

# Full scrape â€” downloads complete history for all companies (~2-4 hours first time)
python scraper/run_daily.py --full-scrape

# Only process new companies (new IPOs/listings)
python scraper/run_daily.py --new-only
```

---

## ğŸ“Š Output Format

### `data/company-wise/{SYMBOL}/prices.csv`
```
date, open, high, low, ltp, percent_change, qty, turnover
```

### `data/company-wise/{SYMBOL}/dividend.csv`
```
fiscal_year, bonus_share, cash_dividend, total_dividend, book_closure_date
```

### `data/company-wise/{SYMBOL}/right-share.csv`
```
ratio, total_units, issue_price, opening_date, closing_date, status, issue_manager
```

### `data/floorsheet_YYYY-MM-DD.csv`
```
date, sn, contract_no, stock_symbol, buyer, seller, quantity, rate, amount
```

---

## âš™ï¸ How Incremental Price Scraping Works

`history.py` reads the latest date from `prices.csv`, then passes it as a `stop_date` to the AJAX paginator. The moment it encounters a record older than `stop_date`, it stops fetching â€” so a daily update fetches only 1-2 pages instead of 70+ pages.

---

## ğŸ“ First-Time Setup

1. Run full price scrape locally (once):
   ```bash
   python scraper/run_daily.py --full-scrape
   ```
2. Commit all data to the repo:
   ```bash
   git add data/
   git commit -m "chore: initial data load"
   git push
   ```
3. GitHub Actions will keep everything updated automatically from then on.
