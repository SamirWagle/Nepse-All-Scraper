<div align="center">

# üìà NEPSE All Scraper

**A free, open-source data pipeline for the Nepal Stock Exchange.**  
Automatically scrapes prices, dividends, right shares, and floorsheet data  
for **337 listed companies** ‚Äî committed to this repo every weekday via GitHub Actions.

[![Daily Scraper](https://github.com/SamirWagle/Nepse-All-Scraper/actions/workflows/daily_scraper.yml/badge.svg)](https://github.com/SamirWagle/Nepse-All-Scraper/actions/workflows/daily_scraper.yml)
![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white)
![Data Source](https://img.shields.io/badge/Source-ShareSansar%20%7C%20Merolagani-orange)
![License](https://img.shields.io/badge/License-Educational-green)
![Companies](https://img.shields.io/badge/Companies-337-purple)

</div>

---

## üì¶ What's Inside

> This repo is **data-first**. Every weekday after NEPSE closes, GitHub Actions scrapes the latest data and commits it directly back to the `data/` folder. No database, no server ‚Äî just flat CSV files you can plug into anything.

| Data | Where | Updated |
|---|---|---|
| OHLC price history | `data/company-wise/{SYMBOL}/prices.csv` | Locally (run once) |
| Dividend history | `data/company-wise/{SYMBOL}/dividend.csv` | Every weekday ‚úÖ |
| Right share history | `data/company-wise/{SYMBOL}/right-share.csv` | Every weekday ‚úÖ |
| Full daily floorsheet | `data/floorsheet_YYYY-MM-DD.csv` + `.json` | Every weekday ‚úÖ |

---

## ÔøΩÔ∏è Repository Layout

```
Nepse-All-Scraper/
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ daily_scraper.yml      # GitHub Actions ‚Äî runs every weekday at 6:30 PM NPT
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ company_list.json          # 337 priority company symbols
‚îÇ   ‚îú‚îÄ‚îÄ company_id_mapping.json    # Symbol ‚Üí ShareSansar internal ID
‚îÇ   ‚îú‚îÄ‚îÄ floorsheet_YYYY-MM-DD.csv  # Daily floorsheet (all trades)
‚îÇ   ‚îú‚îÄ‚îÄ floorsheet_YYYY-MM-DD.json # Same data as JSON
‚îÇ   ‚îî‚îÄ‚îÄ company-wise/
‚îÇ       ‚îî‚îÄ‚îÄ {SYMBOL}/
‚îÇ           ‚îú‚îÄ‚îÄ prices.csv         # Full OHLC price history
‚îÇ           ‚îú‚îÄ‚îÄ dividend.csv       # Dividend history
‚îÇ           ‚îî‚îÄ‚îÄ right-share.csv    # Right share history
‚îÇ
‚îî‚îÄ‚îÄ scraper/
    ‚îú‚îÄ‚îÄ run_github_actions.py      # ‚Üê GitHub Actions entry point
    ‚îú‚îÄ‚îÄ run_daily.py               # ‚Üê Local price scraper CLI
    ‚îî‚îÄ‚îÄ core/
        ‚îú‚îÄ‚îÄ daily.py               # Orchestrates price scraping
        ‚îú‚îÄ‚îÄ daily_prices.py        # Daily price summary updater
        ‚îú‚îÄ‚îÄ floorsheet.py          # Floorsheet scraper (merolagani.com)
        ‚îî‚îÄ‚îÄ history.py             # OHLC price history scraper
```

---

## ü§ñ Automation ‚Äî GitHub Actions

The workflow [`.github/workflows/daily_scraper.yml`](.github/workflows/daily_scraper.yml) runs automatically **every weekday (Mon‚ÄìFri) at 6:30 PM Nepal time** (12:45 UTC), right after NEPSE closes.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              GitHub Actions ‚Äî Daily Run              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Dividends  ‚îÇ  Updates dividend.csv (all 337)        ‚îÇ
‚îÇ Right Shares‚îÇ  Updates right-share.csv (all 337)     ‚îÇ
‚îÇ  Floorsheet ‚îÇ  Full day's trades from merolagani.com ‚îÇ
‚îÇ  Commit     ‚îÇ  git push ‚Üí data/ auto-updated in repo ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Trigger manually**: `GitHub ‚Üí Actions ‚Üí Daily Scraper ‚Üí Run workflow`

---

## ‚ö° Quickstart

### Prerequisites
```bash
pip install requests beautifulsoup4
```

### Run the same scrape as GitHub Actions (dividends + right shares + floorsheet)
```bash
# All three in one go
python scraper/run_github_actions.py

# Or individually
python scraper/run_github_actions.py --dividends
python scraper/run_github_actions.py --right-shares
python scraper/run_github_actions.py --floorsheet

# Test floorsheet with limited pages (faster)
python scraper/run_github_actions.py --floorsheet --max-pages 3
```

### Scrape full OHLC price history (local only, first-time)
```bash
# Full history for all 337 companies ‚Äî takes ~2-4 hours on first run
python scraper/run_daily.py --full-scrape

# Incremental ‚Äî only fetches newer records than what's already in prices.csv
python scraper/run_daily.py --incremental

# Only process newly listed companies (new IPOs)
python scraper/run_daily.py --new-only
```

> **Why are prices local-only?**  
> Price scraping needs the existing `prices.csv` files to know where to stop (incremental logic). Run it locally once, push the data, then automation handles daily updates.

---

## ÔøΩ Data Formats

<details>
<summary><strong>prices.csv</strong></summary>

```
date, open, high, low, ltp, percent_change, qty, turnover
2024-01-15, 1200, 1250, 1190, 1240, +1.5%, 3400, 4216000
```

</details>

<details>
<summary><strong>dividend.csv</strong></summary>

```
fiscal_year, bonus_share, cash_dividend, total_dividend, book_closure_date
2079/80, 10%, 5%, 15%, 2023-12-01
```

</details>

<details>
<summary><strong>right-share.csv</strong></summary>

```
ratio, total_units, issue_price, opening_date, closing_date, status, issue_manager
1:1, 5000000, 100, 2023-11-01, 2023-11-15, Closed, XYZ Capital
```

</details>

<details>
<summary><strong>floorsheet_YYYY-MM-DD.csv</strong></summary>

```
date, sn, contract_no, stock_symbol, buyer, seller, quantity, rate, amount
2024-01-15, 1, 100012345, ADBL, 21, 42, 500, 1240, 620000
```

</details>

---

## ‚öôÔ∏è How Incremental Scraping Works

```
prices.csv                    ShareSansar AJAX
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Latest date: 2024-01-10  ‚Üí   Stop fetching when
                              record date ‚â§ 2024-01-10

Result: Only 1-2 pages fetched instead of 70+
```

`history.py` reads the most recent date from the existing `prices.csv`, passes it as a `stop_date` to the paginator, and halts the moment it hits older data. This makes daily updates take seconds instead of hours.

---

## üöÄ First-Time Setup

```bash
# 1. Clone the repo
git clone https://github.com/SamirWagle/Nepse-All-Scraper.git
cd Nepse-All-Scraper

# 2. Install dependencies
pip install requests beautifulsoup4

# 3. Run the full price history scrape (one-time, takes 2-4 hours)
python scraper/run_daily.py --full-scrape

# 4. Push everything to the repo
git add data/
git commit -m "chore: initial data load"
git push
```

From that point on, GitHub Actions handles everything automatically every weekday. ‚úÖ

---

## üó∫Ô∏è Roadmap

- [x] **Phase 1** ‚Äî NEPSE Scraper (prices, dividends, right shares, floorsheet)
- [x] **Phase 2** ‚Äî GitHub Actions automation + incremental updates
- [ ] **Phase 3** ‚Äî Frontend / API layer

Want to help build Phase 3? **PRs are welcome.**

---

## ‚ö†Ô∏è Disclaimer

> This project is for **educational purposes only**.  
> Data is sourced from publicly available websites (ShareSansar, Merolagani).  
> Not financial advice. Do your own research before making investment decisions.

---

<div align="center">

Made with ‚ù§Ô∏è for the Nepali investor community

</div>